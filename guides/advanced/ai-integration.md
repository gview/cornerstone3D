# AI é›†æˆæŒ‡å—

**ç›®æ ‡**: å­¦ä¹ å¦‚ä½•åœ¨ Cornerstone3D åº”ç”¨ä¸­é›†æˆ AI è¾…åŠ©è¯Šæ–­åŠŸèƒ½

**é¢„è®¡æ—¶é—´**: 70 åˆ†é’Ÿ

**éš¾åº¦**: é«˜çº§

**å‰ç½®è¦æ±‚**:
- [x] å·²å®Œæˆ [ç¬¬ä¸€ä¸ªå½±åƒæŸ¥çœ‹å™¨](../getting-started/first-viewer.md)
- [x] å·²å®Œæˆ [æ ‡æ³¨å·¥å…·](./annotations.md)
- [x] äº†è§£æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ åŸºç¡€
- [x] æ‹¥æœ‰ AI æ¨¡å‹æˆ– API

---

## æ¦‚è¿°

AI è¾…åŠ©è¯Šæ–­æ˜¯åŒ»å­¦å½±åƒçš„å‰æ²¿æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜è¯Šæ–­æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚Cornerstone3D æä¾›äº†çµæ´»çš„æ¶æ„ï¼Œæ”¯æŒé›†æˆå„ç§ AI æ¨¡å‹å’Œ APIã€‚

**ä¸»è¦åº”ç”¨åœºæ™¯**:
- ğŸ¯ **ç—…ç¶æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹è‚¿ç˜¤ã€ç»“èŠ‚ç­‰ç—…ç¶
- ğŸ“ **è‡ªåŠ¨åˆ†å‰²**: åˆ†å‰²å™¨å®˜ã€è‚¿ç˜¤ç­‰ç»“æ„
- ğŸ·ï¸ **è‡ªåŠ¨æ ‡æ³¨**: ç”Ÿæˆæµ‹é‡æ ‡æ³¨å’Œè¯Šæ–­æŠ¥å‘Š
- ğŸ” **è´¨é‡è¯„ä¼°**: è¯„ä¼°å½±åƒè´¨é‡ã€æ£€æµ‹ä¼ªå½±
- ğŸ“Š **ç–¾ç—…åˆ†ç±»**: AI è¾…åŠ©ç–¾ç—…è¯Šæ–­å’Œåˆ†ç±»

**é›†æˆæ–¹å¼**:
- ğŸ”Œ **REST API**: è°ƒç”¨äº‘ç«¯ AI æœåŠ¡
- ğŸ¤– **ONNX Runtime**: åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ ONNX æ¨¡å‹
- ğŸ§  **TensorFlow.js**: åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ TensorFlow æ¨¡å‹
- ğŸ“¦ **æœ¬åœ°æœåŠ¡**: è¿æ¥æœ¬åœ° AI æ¨ç†æœåŠ¡å™¨

---

## AI é›†æˆæ¶æ„

### æ¶æ„å›¾

```mermaid
graph TB
    A[Cornerstone3D Viewport] --> B[å½±åƒæ•°æ®]
    B --> C[é¢„å¤„ç†]
    C --> D{AI æ¨¡å‹/API}
    D --> E[äº‘ç«¯ REST API]
    D --> F[æœ¬åœ° ONNX æ¨¡å‹]
    D --> G[TensorFlow.js]
    D --> E
    D --> F
    D --> G
    E --> H[AI ç»“æœ]
    F --> H
    G --> H
    H --> I[åå¤„ç†]
    I --> J[æ ‡æ³¨/åˆ†å‰²/æŠ¥å‘Š]
    J --> A
```

**å…³é”®ç»„ä»¶**:

- **é¢„å¤„ç†**: å½±åƒæ•°æ®å‡†å¤‡ï¼ˆå½’ä¸€åŒ–ã€è£å‰ªç­‰ï¼‰
- **AI æ¨¡å‹**: æ¨ç†å¼•æ“ï¼ˆAPI æˆ–æœ¬åœ°æ¨¡å‹ï¼‰
- **åå¤„ç†**: ç»“æœè§£æå’Œå¯è§†åŒ–
- **ç»“æœå±•ç¤º**: æ ‡æ³¨ã€åˆ†å‰²æ©è†œã€æŠ¥å‘Š

---

## 2. ä½¿ç”¨ REST API é›†æˆ AI

### 2.1 åŸºç¡€ REST API è°ƒç”¨

```typescript
// è°ƒç”¨ AI REST API è¿›è¡Œç—…ç¶æ£€æµ‹
const detectLesions = async (imageId: string): Promise<Lesion[]> => {
  try {
    // 1. è·å–å½±åƒæ•°æ®
    const image = await imageLoader.loadImage(imageId);
    const imageData = image.getPixelData();

    // 2. å‡†å¤‡è¯·æ±‚æ•°æ®
    const requestPayload = {
      imageId,
      imageData: Array.from(imageData),
      width: image.width,
      height: image.height,
      pixelSpacing: image.pixelSpacing,
    };

    // 3. è°ƒç”¨ AI API
    const response = await fetch('https://ai-service.com/api/detect', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${AI_API_KEY}`,
      },
      body: JSON.stringify(requestPayload),
    });

    if (!response.ok) {
      throw new Error(`AI API è¯·æ±‚å¤±è´¥: ${response.statusText}`);
    }

    // 4. è§£æç»“æœ
    const result = await response.json();
    return result.lesions;

  } catch (error) {
    console.error('AI ç—…ç¶æ£€æµ‹å¤±è´¥:', error);
    return [];
  }
};

// ä½¿ç”¨ç¤ºä¾‹
const lesions = await detectLesions(imageId);
console.log('æ£€æµ‹åˆ°', lesions.length, 'ä¸ªç—…ç¶');
```

### 2.2 æ˜¾ç¤º AI æ£€æµ‹ç»“æœ

```typescript
import { RectangleROITool } from '@cornerstonejs/tools';
import { annotationState } from '@cornerstonejs/tools';

// å°† AI æ£€æµ‹ç»“æœè½¬æ¢ä¸ºæ ‡æ³¨
const displayAILesions = async (lesions: Lesion[], viewportId: string) => {
  const toolGroup = ToolGroupManager.getToolGroup('myToolGroup');

  // ç¡®ä¿çŸ©å½¢ ROI å·¥å…·å·²æ¿€æ´»
  toolGroup.addTool(RectangleROITool.toolName);

  // ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„ç—…ç¶åˆ›å»ºæ ‡æ³¨
  for (const lesion of lesions) {
    const annotation = {
      metadata: {
        toolName: RectangleROITool.toolName,
        viewportId,
      },
      data: {
        rectangle: {
          x: lesion.bbox.x,
          y: lesion.bbox.y,
          width: lesion.bbox.width,
          height: lesion.bbox.height,
        },
        label: {
          value: `AI: ${lesion.type} (${(lesion.confidence * 100).toFixed(1)}%)`,
        },
      },
    };

    // æ·»åŠ æ ‡æ³¨åˆ°è§†å£
    await annotationState.addAnnotation(annotation);
  }

  // æ¸²æŸ“è§†å£
  const viewport = renderingEngine.getViewport(viewportId);
  viewport.render();
};

// ä½¿ç”¨ç¤ºä¾‹
const lesions = await detectLesions(imageId);
await displayAILesions(lesions, 'my-viewport');
```

---

## 3. ä½¿ç”¨ ONNX Runtime

ONNX Runtime å…è®¸åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ AI æ¨¡å‹ï¼Œæ— éœ€è°ƒç”¨è¿œç¨‹ APIã€‚

### 3.1 å®‰è£… ONNX Runtime

```bash
yarn add onnxruntime-web
```

### 3.2 åŠ è½½å’Œè¿è¡Œ ONNX æ¨¡å‹

```typescript
import * as ort from 'onnxruntime-web';

// åˆå§‹åŒ– ONNX Runtime
await ort.env.wasm.numThreads(); // è®¾ç½®çº¿ç¨‹æ•°

// åŠ è½½ ONNX æ¨¡å‹
const loadModel = async (modelPath: string): Promise<ort.InferenceSession> => {
  const session = await ort.InferenceSession.create(modelPath);
  return session;
};

// é¢„å¤„ç†å½±åƒæ•°æ®
const preprocessImage = (image: Types.IImage): ort.Tensor => {
  const imageData = image.getPixelData();

  // å½’ä¸€åŒ–åˆ° [0, 1]
  const normalizedData = new Float32Array(imageData.length);
  for (let i = 0; i < imageData.length; i++) {
    normalizedData[i] = imageData[i] / 4096; // å‡è®¾ 12-bit CT
  }

  // åˆ›å»º ONNX Tensor
  const tensor = new ort.Tensor('float32', normalizedData, [1, 1, image.height, image.width]);

  return tensor;
};

// è¿è¡Œæ¨¡å‹æ¨ç†
const runInference = async (
  session: ort.InferenceSession,
  image: Types.IImage
): Promise<any> => {
  // é¢„å¤„ç†
  const inputTensor = preprocessImage(image);

  // å‡†å¤‡è¾“å…¥
  const feeds = { input: inputTensor };

  // è¿è¡Œæ¨ç†
  const results = await session.run(feeds);

  return results;
};

// ä½¿ç”¨ç¤ºä¾‹
const detectWithONNX = async (imageId: string) => {
  // åŠ è½½æ¨¡å‹
  const session = await loadModel('/models/lesion-detection.onnx');

  // åŠ è½½å½±åƒ
  const image = await imageLoader.loadImage(imageId);

  // è¿è¡Œæ¨ç†
  const results = await runInference(session, image);

  // è§£æç»“æœ
  const detections = results.output.data; // å‡è®¾è¾“å‡ºæ˜¯æ£€æµ‹ç»“æœ
  console.log('æ£€æµ‹ç»“æœ:', detections);

  return detections;
};
```

### 3.3 æ˜¾ç¤º ONNX æ¨¡å‹ç»“æœ

```typescript
// è§£æ ONNX æ¨¡å‹è¾“å‡º
const parseDetectionResults = (results: any): Detection[] => {
  const detections = [];

  // å‡è®¾è¾“å‡ºæ ¼å¼: [num_detections, 6] (x, y, width, height, confidence, class)
  const data = results.output.data as Float32Array;
  const numDetections = data.length / 6;

  for (let i = 0; i < numDetections; i++) {
    const offset = i * 6;
    detections.push({
      bbox: {
        x: data[offset],
        y: data[offset + 1],
        width: data[offset + 2],
        height: data[offset + 3],
      },
      confidence: data[offset + 4],
      classId: Math.round(data[offset + 5]),
    });
  }

  return detections;
};

// æ˜¾ç¤ºæ£€æµ‹ç»“æœ
const displayONNXDetections = async (imageId: string, viewportId: string) => {
  // è¿è¡Œ ONNX æ¨¡å‹
  const results = await detectWithONNX(imageId);

  // è§£æç»“æœ
  const detections = parseDetectionResults(results);

  // æ˜¾ç¤ºä¸ºæ ‡æ³¨
  await displayAILesions(detections, viewportId);
};
```

---

## 4. ä½¿ç”¨ TensorFlow.js

TensorFlow.js æ˜¯å¦ä¸€ä¸ªæµè¡Œçš„æµè§ˆå™¨ç«¯æœºå™¨å­¦ä¹ åº“ã€‚

### 4.1 å®‰è£… TensorFlow.js

```bash
yarn add @tensorflow/tfjs
```

### 4.2 åŠ è½½å’Œè¿è¡Œ TensorFlow.js æ¨¡å‹

```typescript
import * as tf from '@tensorflow/tfjs';

// åŠ è½½ TensorFlow.js æ¨¡å‹
const loadTFJSModel = async (modelPath: string): Promise<tf.GraphModel> => {
  const model = await tf.loadGraphModel(modelPath);
  return model;
};

// é¢„å¤„ç†å½±åƒæ•°æ®
const preprocessImageForTF = (image: Types.IImage): tf.Tensor => {
  const imageData = image.getPixelData();

  // å½’ä¸€åŒ–åˆ° [0, 1]
  const normalizedData = tf.tensor(imageData, [image.height, image.width, 1]);
  const normalized = normalizedData.div(255);

  return normalized;
};

// è¿è¡Œæ¨¡å‹æ¨ç†
const runTFJSInference = async (
  model: tf.GraphModel,
  image: Types.IImage
): Promise<tf.Tensor> => {
  // é¢„å¤„ç†
  const inputTensor = preprocessImageForTF(image);

  // æ·»åŠ æ‰¹æ¬¡ç»´åº¦
  const batched = inputTensor.expandDims(0);

  // è¿è¡Œæ¨ç†
  const results = await model.executeAsync(batched) as tf.Tensor;

  // æ¸…ç†ä¸­é—´å¼ é‡
  inputTensor.dispose();
  batched.dispose();

  return results;
};

// ä½¿ç”¨ç¤ºä¾‹
const segmentWithTFJS = async (imageId: string) => {
  // åŠ è½½æ¨¡å‹
  const model = await loadTFJSModel('/models/segmentation/model.json');

  // åŠ è½½å½±åƒ
  const image = await imageLoader.loadImage(imageId);

  // è¿è¡Œæ¨ç†
  const segmentationMask = await runTFJSInference(model, image);

  // åå¤„ç†åˆ†å‰²æ©è†œ
  const maskData = await segmentationMask.data();
  console.log('åˆ†å‰²æ©è†œ:', maskData);

  // æ¸…ç†å¼ é‡
  segmentationMask.dispose();

  return maskData;
};
```

---

## 5. AI è¾…åŠ©åˆ†å‰²

### 5.1 æ˜¾ç¤ºåˆ†å‰²æ©è†œ

```typescript
// å°†åˆ†å‰²æ©è†œå åŠ åˆ°å½±åƒä¸Š
const displaySegmentationMask = async (
  imageId: string,
  maskData: Uint8Array,
  viewportId: string
) => {
  // è·å–è§†å£
  const viewport = renderingEngine.getViewport(viewportId);

  // è·å–å½±åƒ
  const image = await imageLoader.loadImage(imageId);

  // åˆ›å»ºå½©è‰²æ©è†œ
  const coloredMask = new Uint8Array(maskData.length * 4);
  for (let i = 0; i < maskData.length; i++) {
    const offset = i * 4;

    if (maskData[i] === 1) { // å™¨å®˜/è‚¿ç˜¤
      coloredMask[offset] = 255;     // R
      coloredMask[offset + 1] = 0;   // G
      coloredMask[offset + 2] = 0;   // B
      coloredMask[offset + 3] = 128; // A (50% é€æ˜)
    } else { // èƒŒæ™¯
      coloredMask[offset] = 0;
      coloredMask[offset + 1] = 0;
      coloredMask[offset + 2] = 0;
      coloredMask[offset + 3] = 0;   // å®Œå…¨é€æ˜
    }
  }

  // åˆ›å»ºå›¾åƒå¯¹è±¡
  const maskImage = new ImageData(
    new Uint8ClampedArray(coloredMask),
    image.width,
    image.height
  );

  // åœ¨ Canvas ä¸Šç»˜åˆ¶æ©è†œ
  const canvas = document.createElement('canvas');
  canvas.width = image.width;
  canvas.height = image.height;
  const ctx = canvas.getContext('2d')!;
  ctx.putImageData(maskImage, 0, 0);

  // å°†æ©è†œæ·»åŠ åˆ°è§†å£
  // å…·ä½“å®ç°å–å†³äº Cornerstone3D ç‰ˆæœ¬å’Œ API
};
```

### 5.2 äº¤äº’å¼åˆ†å‰²

```typescript
// å…è®¸ç”¨æˆ·è°ƒæ•´åˆ†å‰²é˜ˆå€¼
const adjustSegmentationThreshold = (maskData: Uint8Array, threshold: number): Uint8Array => {
  const adjustedMask = new Uint8Array(maskData.length);

  for (let i = 0; i < maskData.length; i++) {
    // åº”ç”¨é˜ˆå€¼
    adjustedMask[i] = maskData[i] >= threshold ? 1 : 0;
  }

  return adjustedMask;
};

// ä½¿ç”¨æ»‘å—è°ƒæ•´é˜ˆå€¼
const thresholdSlider = document.getElementById('segmentation-threshold');

thresholdSlider.addEventListener('input', async (event) => {
  const threshold = parseFloat((event.target as HTMLInputElement).value);

  // è°ƒæ•´åˆ†å‰²
  const adjustedMask = adjustSegmentationThreshold(originalMaskData, threshold);

  // é‡æ–°æ˜¾ç¤º
  await displaySegmentationMask(imageId, adjustedMask, viewportId);
});
```

---

## 6. å®Œæ•´ç¤ºä¾‹ï¼šAI ç—…ç¶æ£€æµ‹ç»„ä»¶

```typescript
import React, { useState } from 'react';
import { renderingEngine, imageLoader } from '../cornerstone';

interface Lesion {
  bbox: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  confidence: number;
  type: string;
}

const AILesionDetection: React.FC = () => {
  const [isDetecting, setIsDetecting] = useState(false);
  const [lesions, setLesions] = useState<Lesion[]>([]);

  // è°ƒç”¨ AI API æ£€æµ‹ç—…ç¶
  const detectLesions = async (imageId: string): Promise<Lesion[]> => {
    try {
      // 1. è·å–å½±åƒæ•°æ®
      const image = await imageLoader.loadImage(imageId);
      const imageData = image.getPixelData();

      // 2. è°ƒç”¨ AI API
      const response = await fetch('/api/ai/detect-lesions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          imageId,
          imageData: Array.from(imageData.slice(0, 1000)), // ç¤ºä¾‹ï¼šåªå‘é€éƒ¨åˆ†æ•°æ®
          width: image.width,
          height: image.height,
        }),
      });

      if (!response.ok) {
        throw new Error(`æ£€æµ‹å¤±è´¥: ${response.statusText}`);
      }

      const result = await response.json();
      return result.lesions;

    } catch (error) {
      console.error('AI æ£€æµ‹å¤±è´¥:', error);
      return [];
    }
  };

  // æ˜¾ç¤ºæ£€æµ‹ç»“æœ
  const displayResults = async (lesions: Lesion[]) => {
    // å°†æ£€æµ‹ç»“æœæ·»åŠ åˆ°è§†å£ä½œä¸ºæ ‡æ³¨
    const { annotationState } = await import('@cornerstonejs/tools');

    for (const lesion of lesions) {
      const annotation = {
        metadata: {
          toolName: 'RectangleROITool',
          viewportId: 'my-viewport',
        },
        data: {
          rectangle: lesion.bbox,
          label: {
            value: `${lesion.type} (${(lesion.confidence * 100).toFixed(1)}%)`,
          },
        },
      };

      await annotationState.addAnnotation(annotation);
    }

    // æ¸²æŸ“è§†å£
    const viewport = renderingEngine.getViewport('my-viewport');
    viewport.render();
  };

  // å¤„ç†æ£€æµ‹æŒ‰é’®ç‚¹å‡»
  const handleDetect = async () => {
    setIsDetecting(true);
    setLesions([]);

    try {
      // è·å–å½“å‰å½±åƒ ID
      const viewport = renderingEngine.getViewport('my-viewport');
      const imageId = viewport.getCurrentImageId();

      // è°ƒç”¨ AI æ£€æµ‹
      const detectedLesions = await detectLesions(imageId);

      // æ›´æ–°çŠ¶æ€
      setLesions(detectedLesions);

      // æ˜¾ç¤ºç»“æœ
      await displayResults(detectedLesions);

    } catch (error) {
      console.error('æ£€æµ‹å¤±è´¥:', error);
      alert('æ£€æµ‹å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯ä¿¡æ¯');
    } finally {
      setIsDetecting(false);
    }
  };

  return (
    <div style={{ padding: '16px', border: '1px solid #ccc' }}>
      <h3>AI ç—…ç¶æ£€æµ‹</h3>

      <button
        onClick={handleDetect}
        disabled={isDetecting}
        style={{ marginBottom: '16px' }}
      >
        {isDetecting ? 'æ£€æµ‹ä¸­...' : 'å¼€å§‹æ£€æµ‹'}
      </button>

      {lesions.length > 0 && (
        <div>
          <h4>æ£€æµ‹ç»“æœ ({lesions.length})</h4>
          <ul>
            {lesions.map((lesion, index) => (
              <li key={index}>
                {lesion.type} - ç½®ä¿¡åº¦: {(lesion.confidence * 100).toFixed(1)}%
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
};

export default AILesionDetection;
```

---

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 æ‰¹é‡æ¨ç†

```typescript
// æ‰¹é‡å¤„ç†å¤šå¼ å½±åƒä»¥æé«˜æ•ˆç‡
const batchDetect = async (imageIds: string[], batchSize: number = 5): Promise<Lesion[][]> => {
  const results = [];

  for (let i = 0; i < imageIds.length; i += batchSize) {
    const batch = imageIds.slice(i, i + batchSize);

    // å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
    const batchResults = await Promise.all(
      batch.map(imageId => detectLesions(imageId))
    );

    results.push(...batchResults);
  }

  return results;
};
```

### 7.2 ä½¿ç”¨ Web Worker

```typescript
// åœ¨ Web Worker ä¸­è¿è¡Œ AI æ¨ç†
// worker.js
self.addEventListener('message', async (event) => {
  const { imageId, modelPath } = event.data;

  // åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
  if (!self.model) {
    self.model = await loadModel(modelPath);
  }

  // è¿è¡Œæ¨ç†
  const results = await runInference(self.model, imageId);

  // å‘é€ç»“æœå›ä¸»çº¿ç¨‹
  self.postMessage({ results });
});
```

---

## 8. å¸¸è§é—®é¢˜

### Q1: AI æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
- âœ… ä½¿ç”¨ Web Worker åœ¨åå°çº¿ç¨‹è¿è¡Œæ¨ç†
- âœ… é™ä½è¾“å…¥åˆ†è¾¨ç‡
- âœ… ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆINT8 é‡åŒ–ï¼‰
- âœ… ä½¿ç”¨æ‰¹é‡æ¨ç†

### Q2: AI æ£€æµ‹ç»“æœä¸å‡†ç¡®ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
- âœ… æ£€æŸ¥å½±åƒé¢„å¤„ç†æ˜¯å¦æ­£ç¡®
- âœ… æ£€æŸ¥æ¨¡å‹æ˜¯å¦é€‚åˆå½“å‰å½±åƒç±»å‹
- âœ… è°ƒæ•´æ¨ç†é˜ˆå€¼
- âœ… ä½¿ç”¨æ›´é€‚åˆçš„æ¨¡å‹

### Q3: å†…å­˜æº¢å‡ºï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
- âœ… é™ä½è¾“å…¥åˆ†è¾¨ç‡
- âœ… ä½¿ç”¨æ¨¡å‹é‡åŒ–
- âœ… åŠæ—¶æ¸…ç†æ¨¡å‹å’Œå¼ é‡
- âœ… ä½¿ç”¨æµå¼æ¨ç†ï¼ˆåˆ†å—å¤„ç†ï¼‰

---

## 9. AI åº”ç”¨åœºæ™¯ç¤ºä¾‹

### 9.1 è‚ºç»“èŠ‚æ£€æµ‹

```typescript
// è‚ºç»“èŠ‚ AI æ£€æµ‹
const detectLungNodules = async (imageId: string) => {
  const noduleDetections = await detectLesions(imageId);

  // è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
  const highConfidenceNodules = noduleDetections.filter(
    detection => detection.confidence > 0.8
  );

  // æŒ‰å¤§å°æ’åº
  highConfidenceNodules.sort((a, b) => {
    const areaA = a.bbox.width * a.bbox.height;
    const areaB = b.bbox.width * b.bbox.height;
    return areaB - areaA;
  });

  return highConfidenceNodules;
};
```

### 9.2 éª¨æŠ˜æ£€æµ‹

```typescript
// éª¨æŠ˜ AI æ£€æµ‹
const detectFractures = async (imageId: string) => {
  const response = await fetch('/api/ai/detect-fractures', {
    method: 'POST',
    body: JSON.stringify({ imageId }),
  });

  const fractures = await response.json();

  // æ˜¾ç¤ºéª¨æŠ˜ä½ç½®æ ‡æ³¨
  fractures.forEach(fracture => {
    addFractureAnnotation(fracture);
  });

  return fractures;
};
```

---

## 10. ä¸‹ä¸€æ­¥

- ğŸ› ï¸ [è‡ªå®šä¹‰å·¥å…·](./custom-tools.md) - å¼€å‘è‡ªå®šä¹‰ AI å·¥å…·
- âš¡ [æ€§èƒ½ä¼˜åŒ–](./performance-optimization.md) - ä¼˜åŒ– AI æ¨ç†æ€§èƒ½
- ğŸ“Š [å¤šè§†å£åŒæ­¥](./multi-viewport.md) - åœ¨å¤šè§†å£ä¸­æ˜¾ç¤º AI ç»“æœ

---

## ç›¸å…³èµ„æº

- ğŸ“š [ONNX Runtime æ–‡æ¡£](https://onnxruntime.ai/docs/)
- ğŸ“š [TensorFlow.js æ–‡æ¡£](https://www.tensorflow.org/js)
- ğŸ’» [ç¤ºä¾‹é¡¹ç›® - AI é›†æˆ](../examples/advanced-viewer/)
- ğŸ” [Cornerstone3D AI ç¤ºä¾‹](https://www.cornerstonejs.org/examples/ai-integration)

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** æŸ¥çœ‹ [æ•…éšœæ’æŸ¥æ–‡æ¡£](../troubleshooting/common-errors.md)
